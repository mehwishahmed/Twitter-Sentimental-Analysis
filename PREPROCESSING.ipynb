{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1805866d-a6ec-4fff-8adc-165d015b5d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mehwishahmed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mehwishahmed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mehwishahmed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Tweet  \\\n",
      "0  I will finish Borderlands 2 today. I have some...   \n",
      "1  I'm going to finish Borderlands 2 today. I hav...   \n",
      "2  Going to finish up Borderlands 2 today. I've g...   \n",
      "3  Going to finish finish cleaning up Borderlands...   \n",
      "4  Going to finish up volume 2 today. I've got so...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  i will finish borderlands  today i have some n...   \n",
      "1  im going to finish borderlands  today i have s...   \n",
      "2  going to finish up borderlands  today ive got ...   \n",
      "3  going to finish finish cleaning up borderlands...   \n",
      "4  going to finish up volume  today ive got some ...   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [i, will, finish, borderlands, today, i, have,...   \n",
      "1  [im, going, to, finish, borderlands, today, i,...   \n",
      "2  [going, to, finish, up, borderlands, today, iv...   \n",
      "3  [going, to, finish, finish, cleaning, up, bord...   \n",
      "4  [going, to, finish, up, volume, today, ive, go...   \n",
      "\n",
      "                                       filtered_text  \\\n",
      "0  [finish, borderlands, today, new, commands, se...   \n",
      "1  [im, going, finish, borderlands, today, new, c...   \n",
      "2  [going, finish, borderlands, today, ive, got, ...   \n",
      "3  [going, finish, finish, cleaning, borderlands,...   \n",
      "4  [going, finish, volume, today, ive, got, aweso...   \n",
      "\n",
      "                                     lemmatized_text  \\\n",
      "0  [finish, borderland, today, new, command, set,...   \n",
      "1  [im, going, finish, borderland, today, new, co...   \n",
      "2  [going, finish, borderland, today, ive, got, n...   \n",
      "3  [going, finish, finish, cleaning, borderland, ...   \n",
      "4  [going, finish, volume, today, ive, got, aweso...   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0  [finish, borderland, today, new, command, set,...   \n",
      "1  [im, go, finish, borderland, today, new, comma...   \n",
      "2  [go, finish, borderland, today, ive, got, new,...   \n",
      "3  [go, finish, finish, clean, borderland, today,...   \n",
      "4  [go, finish, volum, today, ive, got, awesom, s...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  finish borderland today new command set look f...  \n",
      "1  im go finish borderland today new command inst...  \n",
      "2  go finish borderland today ive got new event s...  \n",
      "3  go finish finish clean borderland today ive go...  \n",
      "4  go finish volum today ive got awesom server se...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'twitter_training.csv'  # Update this path if necessary\n",
    "tweets_df = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "# Ensure all tweets are strings and handle missing values\n",
    "tweets_df['Tweet'] = tweets_df['Tweet'].astype(str).fillna('')\n",
    "\n",
    "# Define preprocessing functions\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n",
    "    tweet = re.sub(r'http\\S+', 'URL', tweet)\n",
    "    tweet = re.sub(r'@\\w+', 'MENTION', tweet)\n",
    "    tweet = re.sub(r'#\\w+', 'HASHTAG', tweet)\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "    return word_tokenize(tweet)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tweet(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def stem_tweet(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tweets_df['cleaned_text'] = tweets_df['Tweet'].apply(clean_tweet)\n",
    "tweets_df['tokenized_text'] = tweets_df['cleaned_text'].apply(tokenize_tweet)\n",
    "tweets_df['filtered_text'] = tweets_df['tokenized_text'].apply(remove_stopwords)\n",
    "tweets_df['lemmatized_text'] = tweets_df['filtered_text'].apply(lemmatize_tweet)\n",
    "tweets_df['stemmed_text'] = tweets_df['lemmatized_text'].apply(stem_tweet)\n",
    "\n",
    "# Combine words back into a sentence for the final preprocessed text\n",
    "tweets_df['preprocessed_text'] = tweets_df['stemmed_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Display the final preprocessed dataset\n",
    "print(tweets_df[['Tweet', 'cleaned_text', 'tokenized_text', 'filtered_text', 'lemmatized_text', 'stemmed_text', 'preprocessed_text']].head())\n",
    "\n",
    "# Save the preprocessed dataset to a CSV file\n",
    "output_file_path = 'preprocessed_tweets_dataset.csv'\n",
    "tweets_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d69665-05e7-4fc9-b9b0-f9bf4c9dfd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
