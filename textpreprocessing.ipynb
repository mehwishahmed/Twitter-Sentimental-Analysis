{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr0KT9ZXkTGd",
        "outputId": "b8b2b1a9-65f9-494a-91ed-2de0c0002975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tweet:\n",
            "Wow! The new iPhone 14 is amazing! üòç #AppleEvent @Apple Check out https://example.com\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Sample tweet\n",
        "tweet = \"Wow! The new iPhone 14 is amazing! üòç #AppleEvent @Apple Check out https://example.com\"\n",
        "\n",
        "print(\"Original Tweet:\")\n",
        "print(tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. Remove Unicode Strings and Noise\n",
        "tweet = re.sub(r'[^\\x00-\\x7F]+', ' ', tweet)\n",
        "print(\"\\nAfter removing Unicode strings and noise:\")\n",
        "print(tweet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtwiIDJAlNd6",
        "outputId": "270ea0ab-2d43-4d3c-d7b4-2843b66c9391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After removing Unicode strings and noise:\n",
            "Wow! The new iPhone 14 is amazing!   #AppleEvent @Apple Check out https://example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Replace URLs, User Mentions and Hashtags\n",
        "tweet = re.sub(r'http\\S+', 'URL', tweet)\n",
        "tweet = re.sub(r'@\\w+', 'MENTION', tweet)\n",
        "tweet = re.sub(r'#\\w+', 'HASHTAG', tweet)\n",
        "print(\"\\nAfter replacing URLs, mentions, and hashtags:\")\n",
        "print(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDPSWopulviR",
        "outputId": "62974c2e-a0c5-4463-9481-8f15e7bc0548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After replacing URLs, mentions, and hashtags:\n",
            "Wow! The new iPhone 14 is amazing!   HASHTAG MENTION Check out URL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Remove Numbers\n",
        "tweet = re.sub(r'\\d+', '', tweet)\n",
        "print(\"\\nAfter removing numbers:\")\n",
        "print(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BNVjZchlygx",
        "outputId": "e75ed5fb-218f-42ab-8c8e-a7d3096487ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After removing numbers:\n",
            "Wow! The new iPhone  is amazing!   HASHTAG MENTION Check out URL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Remove Punctuation\n",
        "tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "print(\"\\nAfter removing punctuation:\")\n",
        "print(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o7fdstml38a",
        "outputId": "0e3d02e0-46e2-4704-8e56-ba6dd0e7c7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After removing punctuation:\n",
            "Wow The new iPhone  is amazing   HASHTAG MENTION Check out URL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Lowercase\n",
        "tweet = tweet.lower()\n",
        "print(\"\\nAfter converting to lowercase:\")\n",
        "print(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZlUBDByl7bg",
        "outputId": "477a8dd1-3578-4ced-aa5b-d442ee647ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After converting to lowercase:\n",
            "wow the new iphone  is amazing   hashtag mention check out url\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0oYj6AAmgCi",
        "outputId": "306b95b3-24c4-4732-bdb7-c0758ac7ac50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Input tweet after converting to lowercase\n",
        "tweet = \"wow the new iphone is amazing hashtag mention check out url\"\n",
        "\n",
        "# Ensure that NLTK's stopwords are available\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Remove Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(tweet)\n",
        "filtered_tweet = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "print(\"After removing stopwords:\")\n",
        "print(filtered_tweet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JFpYxCTl-3f",
        "outputId": "c33eb0b3-9b69-4b71-ae76-326caa5118f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing stopwords:\n",
            "['wow', 'new', 'iphone', 'amazing', 'hashtag', 'mention', 'check', 'url']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizing reduces words to their base or root form."
      ],
      "metadata": {
        "id": "hvE7aKpPpPwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the WordNet corpus\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Provided filtered tweet\n",
        "filtered_tweet = ['wow', 'new', 'iphone', 'amazing', 'hashtag', 'mention', 'check', 'url']\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word in the filtered tweet\n",
        "lemmatized_tweet = [lemmatizer.lemmatize(w) for w in filtered_tweet]\n",
        "\n",
        "# Print the results\n",
        "print(\"\\nAfter lemmatizing:\")\n",
        "print(lemmatized_tweet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzyEJrU-nLSR",
        "outputId": "b5a3fdc2-4e84-45fe-a125-95eb26261bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After lemmatizing:\n",
            "['wow', 'new', 'iphone', 'amazing', 'hashtag', 'mention', 'check', 'url']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PorterStemmer() is an algorithm used for stemming English words. It‚Äôs part of the NLTK library.\n",
        "stemmed_tweet is a list comprehension that applies the stemmer to each word in lemmatized_tweet.\n",
        "stemmer.stem(w) applies the stemming process to each word w.\n",
        "The result, stemmed_tweet, is a list of words where each word has been reduced to its stemmed form.\n",
        "Finally, it prints out the list of stemmed words.\n",
        "This step is often used in text preprocessing for machine learning models where the exact form of a word is less important than the type of action or idea it represents."
      ],
      "metadata": {
        "id": "nTjZ3oUZpX75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tweet = [stemmer.stem(w) for w in lemmatized_tweet]\n",
        "print(\"\\nAfter stemming:\")\n",
        "print(stemmed_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZi7nKtbnwrq",
        "outputId": "c70ecb4a-8a41-406e-a6b7-23bad6474177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After stemming:\n",
            "['wow', 'new', 'iphon', 'amaz', 'hashtag', 'mention', 'check', 'url']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine words back to sentence\n",
        "preprocessed_tweet = ' '.join(stemmed_tweet)\n",
        "print(\"\\nFinal preprocessed tweet:\")\n",
        "print(preprocessed_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgWixKMnnCPh",
        "outputId": "19d1b267-43dc-49e0-fc19-713cfb247824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final preprocessed tweet:\n",
            "wow new iphon amaz hashtag mention check url\n"
          ]
        }
      ]
    }
  ]
}